# CPacket Project - DDoS Attack Detection & Synthetic Data Generation

A comprehensive machine learning pipeline for DDoS attack detection and synthetic network traffic data generation using Large Language Models (LLMs). This project combines traditional machine learning approaches with modern LLM fine-tuning techniques to create realistic synthetic network attack data.

## ğŸ”— Project Overview

This project implements a complete workflow for:

- **Data Extraction**: Extract specific attack patterns from DDoS datasets
- **Data Analysis**: Perform statistical analysis to understand attack characteristics
- **LLM Fine-tuning**: Train language models to generate synthetic network traffic data
- **Data Generation**: Create new synthetic attack data using fine-tuned models
- **Data Validation**: Convert generated data back to structured formats for verification

## ğŸ“ Project Structure

```
â”œâ”€â”€ analysis/                    # Data analysis scripts
â”‚   â””â”€â”€ basic_data_analysis.py  # Statistical analysis of network traffic data
â”œâ”€â”€ data_preparation/           # Data preprocessing for LLM training
â”‚   â””â”€â”€ prepare_data.py        # Convert CSV data to LLM training format
â”œâ”€â”€ datasets/                  # Raw and processed datasets
â”‚   â”œâ”€â”€ attack_tcp_flag_osyn.csv
â”‚   â””â”€â”€ ddos.parquet
â”œâ”€â”€ features/                  # Feature selection results
â”‚   â”œâ”€â”€ anova.json
â”‚   â”œâ”€â”€ extra_tree.json
â”‚   â””â”€â”€ information_gain.json
â”œâ”€â”€ fine-tuning/              # LLM fine-tuning notebooks
â”‚   â”œâ”€â”€ unsloth-train.ipynb   # Training notebook for Google Colab
â”‚   â””â”€â”€ unsloth-inference.ipynb # Inference/generation notebook
â”œâ”€â”€ generated_data/           # LLM-generated synthetic data
â”œâ”€â”€ saved_models/            # Trained ML models and encoders
â”œâ”€â”€ templates/               # LLM instruction templates
â”‚   â””â”€â”€ attack_tcp_flag_osyn/
â”‚       â”œâ”€â”€ extra_tree_instructions.txt
â”‚       â”œâ”€â”€ extra_tree_system_instructions.txt
â”‚       â”œâ”€â”€ extra_tree_user_instructions.txt
â”‚       â”œâ”€â”€ extra_tree.xml
â”‚       â””â”€â”€ xml_to_feature_map.json
â”œâ”€â”€ training_data/           # Processed training data for LLMs
â”œâ”€â”€ utils/                   # Utility scripts
â”‚   â”œâ”€â”€ extract-data-by-activity.py # Extract data by attack type
â”‚   â””â”€â”€ parse_xml.py        # Convert XML output back to CSV
â””â”€â”€ cpacket/                # Python virtual environment
```

## ğŸš€ Complete Workflow

### Step 1: Data Extraction by Activity Type

Extract specific attack patterns from the main DDoS dataset:

```bash
python utils/extract-data-by-activity.py
```

This script:

- Filters the DDoS dataset by specific attack activities
- Applies feature selection (ANOVA, Extra Tree, Information Gain)
- Saves filtered datasets for further analysis

### Step 2: Data Analysis

Analyze the extracted data to understand attack characteristics:

```bash
python analysis/basic_data_analysis.py
```

This script provides:

- Basic dataset statistics (shape, attack types, labels)
- Feature distributions and correlations
- Statistical summaries of network traffic features
- Key insights about attack patterns

### Step 3: Create LLM Instruction Templates

Based on the analysis results, create instruction templates for LLM fine-tuning. Use the statistical insights from Step 2 with AI assistants like ChatGPT or Claude to generate:

- **System Instructions**: Define the AI's role as a cybersecurity expert
- **User Instructions**: Specify the attack generation task
- **XML Templates**: Structure for the output format
- **Feature Mappings**: Map XML elements to CSV columns

Example templates are available in `templates/attack_tcp_flag_osyn/`:

- `extra_tree_system_instructions.txt`
- `extra_tree_user_instructions.txt`
- `extra_tree.xml`
- `xml_to_feature_map.json`

### Step 4: Prepare Training Data

Convert CSV data to LLM training format:

```bash
python data_preparation/prepare_data.py
```

This script:

- Loads the filtered attack dataset
- Combines it with instruction templates
- Generates training data in JSON format suitable for LLM fine-tuning
- Saves the training data to `training_data/`

### Step 5: Fine-tune LLM Models

Use the provided Jupyter notebooks for LLM fine-tuning on Google Colab:

#### Training Notebook (`fine-tuning/unsloth-train.ipynb`)

- Upload your training data from Step 4
- Configure the model (Llama, Mistral, etc.) using Unsloth
- Fine-tune the model on your synthetic data generation task
- Save the trained model

#### Inference Notebook (`fine-tuning/unsloth-inference.ipynb`)

- Load your fine-tuned model
- Generate new synthetic attack data
- Export the generated data as XML files

### Step 6: Parse Generated Data

Convert the LLM-generated XML data back to CSV format:

```bash
python utils/parse_xml.py
```

This script:

- Parses XML files generated by the fine-tuned LLM
- Uses the feature mapping to convert XML to structured CSV
- Saves the final dataset for classification and verification

## ğŸ› ï¸ Requirements

### Python Environment

The project uses a virtual environment located in `cpacket/`. Key dependencies include:

- pandas
- numpy
- scikit-learn
- lxml
- xml.etree.ElementTree
- pathlib

### For LLM Fine-tuning (Google Colab)

- unsloth
- transformers
- torch
- datasets
- trl (Transformer Reinforcement Learning)

## ğŸ“Š Supported Attack Types

Currently supports:

- **TCP-Flag-OSYN**: TCP connection manipulation attacks
- Easily extensible to other DDoS attack patterns

## ğŸ”¬ Machine Learning Features

The project implements multiple feature selection techniques:

- **ANOVA F-test**: Statistical significance testing
- **Extra Trees**: Tree-based feature importance
- **Information Gain**: Information theory-based selection
